# app/llm_analysis.py

import requests
import json

API_URL = "http://localhost:1234/v1/chat/completions"  # LM Studio API

def load_transcript(path="data/transcript.txt"):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def build_prompt(transcript):
    return f"""
–ù–∏–∂–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –≤–∏–¥–µ–æ (Reels/TikTok):

{transcript}

–†–∞–∑–±–µ—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–æ–ª–∏–∫–∞ –∏ –≤–µ—Ä–Ω–∏ –æ—Ç—á—ë—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É:

üéØ **–ö—Ä—é—á–æ–∫** ‚Äî —á—Ç–æ —Ü–µ–ø–ª—è–µ—Ç –≤ –Ω–∞—á–∞–ª–µ?

üîÑ **–†–∞–∑–≤–∏—Ç–∏–µ** ‚Äî –≥–¥–µ —Ä–∞–∑–≤–∏—Ç–∏–µ, –∫–∞–∫ —Ä–∞—Å—Ç—ë—Ç –∏–Ω—Ç—Ä–∏–≥–∞?

üî• **–ö—É–ª—å–º–∏–Ω–∞—Ü–∏—è** ‚Äî –µ—Å—Ç—å –ª–∏ –∫—É–ª—å–º–∏–Ω–∞—Ü–∏—è?

üé¨ **–ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é** ‚Äî –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é?

üé≠ **–≠–º–æ—Ü–∏–∏ –∏ —Ç—Ä–∏–≥–≥–µ—Ä—ã** ‚Äî —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å? —é–º–æ—Ä? –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è?

‚è±Ô∏è **–†–∏—Ç–º –∏ –ø–∞—É–∑—ã** ‚Äî –∫–∞–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ?

‚úÖ **–ê–Ω–∞–ª–∏–∑ –∏ —Å–æ–≤–µ—Ç—ã** ‚Äî —á—Ç–æ —Ö–æ—Ä–æ—à–æ, —á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?

–û—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É. –Ø–∑—ã–∫ ‚Äî —Ä—É—Å—Å–∫–∏–π. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º.
"""

def ask_llm(prompt):
    headers = {"Content-Type": "application/json"}
    payload = {
        "model": "mistral",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "max_tokens": 2048,
    }

    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]

def save_report(content, path="data/report.txt"):
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)

if __name__ == "__main__":
    transcript = load_transcript()
    prompt = build_prompt(transcript)
    result = ask_llm(prompt)
    save_report(result)
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ data/report.txt")
